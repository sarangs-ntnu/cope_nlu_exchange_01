{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Teacher vs. Course Classification\n",
        "Binary classifier to route feedback to the right workflow; includes heuristics, classical models, transformer probe, and cross-domain stress tests.\n"
      ],
      "execution_count": null,
      "outputs": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_PATH = Path('data_feedback.xlsx')\n",
        "df = pd.read_excel(DATA_PATH)\n",
        "print(df.head())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "print('Label counts:')\n",
        "print(df['teacher/course'].value_counts())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 1. Keyword heuristic\n",
        "Useful as a sanity check and for interpretable fallbacks.\n"
      ],
      "execution_count": null,
      "outputs": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "import re\n",
        "\n",
        "def heuristic_label(text):\n",
        "    text_l = text.lower()\n",
        "    if re.search(r'\bteacher\b|sir|madam|teacher's', text_l):\n",
        "        return 'teacher'\n",
        "    if re.search(r'course|syllabus|curriculum', text_l):\n",
        "        return 'course'\n",
        "    return 'teacher'  # default toward teacher feedback\n",
        "\n",
        "heuristic_preds = df['comments'].apply(heuristic_label)\n",
        "from sklearn.metrics import accuracy_score\n",
        "print('Heuristic accuracy:', accuracy_score(df['teacher/course'], heuristic_preds))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 2. TF\u2013IDF + Logistic Regression baseline\n"
      ],
      "execution_count": null,
      "outputs": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    df['comments'], df['teacher/course'], test_size=0.3, random_state=42, stratify=df['teacher/course']\n",
        ")\n",
        "\n",
        "word_lr = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(ngram_range=(1,2), min_df=1)),\n",
        "    ('clf', LogisticRegression(max_iter=200, class_weight='balanced'))\n",
        "])\n",
        "word_lr.fit(X_train, y_train)\n",
        "preds = word_lr.predict(X_val)\n",
        "print(classification_report(y_val, preds))\n",
        "ConfusionMatrixDisplay.from_predictions(y_val, preds, normalize='true', cmap='Blues')\n",
        "plt.title('Teacher vs course baseline')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 3. Character model for spelling robustness\n"
      ],
      "execution_count": null,
      "outputs": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer as CharTfidf\n",
        "\n",
        "char_lr = Pipeline([\n",
        "    ('tfidf', CharTfidf(analyzer='char', ngram_range=(3,5), min_df=1)),\n",
        "    ('clf', LogisticRegression(max_iter=200, class_weight='balanced'))\n",
        "])\n",
        "char_lr.fit(X_train, y_train)\n",
        "char_preds = char_lr.predict(X_val)\n",
        "print(classification_report(y_val, char_preds))\n",
        "ConfusionMatrixDisplay.from_predictions(y_val, char_preds, normalize='true', cmap='Reds')\n",
        "plt.title('Character baseline')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 4. Cross-task generalization stress test\n",
        "Train on positive-sentiment subset, test on neutral/negative (or vice versa) to check robustness to sentiment shift.\n"
      ],
      "execution_count": null,
      "outputs": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "pos_mask = df['sentiment'].str.lower().eq('positive')\n",
        "nonpos_mask = ~pos_mask\n",
        "\n",
        "model = word_lr\n",
        "model.fit(df.loc[pos_mask, 'comments'], df.loc[pos_mask, 'teacher/course'])\n",
        "pos_to_non = accuracy_score(df.loc[nonpos_mask, 'teacher/course'], model.predict(df.loc[nonpos_mask, 'comments']))\n",
        "\n",
        "model.fit(df.loc[nonpos_mask, 'comments'], df.loc[nonpos_mask, 'teacher/course'])\n",
        "non_to_pos = accuracy_score(df.loc[pos_mask, 'teacher/course'], model.predict(df.loc[pos_mask, 'comments']))\n",
        "print({'train_pos_test_nonpos': pos_to_non, 'train_nonpos_test_pos': non_to_pos})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 5. Transformer probe (zero-/few-shot)\n"
      ],
      "execution_count": null,
      "outputs": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "zs = pipeline('zero-shot-classification', model='facebook/bart-large-mnli')\n",
        "labels = ['teacher','course']\n",
        "example = df['comments'].iloc[2]\n",
        "print(zs(example, candidate_labels=labels, hypothesis_template='This feedback is about the {label}.'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 6. Agentic router combining heuristic + model confidence\n"
      ],
      "execution_count": null,
      "outputs": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "probs = word_lr.predict_proba(X_val)\n",
        "threshold = 0.6\n",
        "combined_preds = []\n",
        "for text, p in zip(X_val, probs):\n",
        "    max_p = p.max()\n",
        "    if max_p >= threshold:\n",
        "        combined_preds.append(word_lr.classes_[p.argmax()])\n",
        "    else:\n",
        "        combined_preds.append(heuristic_label(text))\n",
        "\n",
        "print('Hybrid accuracy:', accuracy_score(y_val, combined_preds))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 7. Error analysis with aspect/context tags\n"
      ],
      "execution_count": null,
      "outputs": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "errors = pd.DataFrame({'text': X_val, 'true': y_val, 'pred': preds})\n",
        "errors = errors[errors['true'] != errors['pred']]\n",
        "errors['mentions_teacher'] = errors['text'].str.contains('teacher|sir|madam', case=False)\n",
        "errors['mentions_course'] = errors['text'].str.contains('course|syllabus|curriculum', case=False)\n",
        "print(errors)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}