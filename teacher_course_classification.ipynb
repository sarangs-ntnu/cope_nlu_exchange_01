{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Teacher vs. Course Classification Experiments\n\nGoal: classify feedback as about the **teacher** or the **course**. We progress from keyword baselines to transformers and agentic routing with explainable AI and research-ethics considerations."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import pandas as pd\nfrom pathlib import Path\n\ndf = pd.read_excel(Path('data_feedback.xlsx'))\ndf.head()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Quick heuristic baseline\nRationale: keyword/phrase rules provide a transparent starting point and sanity check."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def keyword_baseline(text):\n    teacher_terms = ['teacher', 'sir', 'he', 'she', 'instructor']\n    course_terms = ['course', 'syllabus', 'practical', 'module']\n    t_hits = sum(term in text.lower() for term in teacher_terms)\n    c_hits = sum(term in text.lower() for term in course_terms)\n    return 'teacher' if t_hits >= c_hits else 'course'\n\ndf['kw_pred'] = df['comments'].apply(keyword_baseline)\n(df['kw_pred'] == df['teacher/course']).mean()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Baseline: TF\u2013IDF + Linear Classifier\nRationale: fast and interpretable with clear feature weights."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report\n\nX_train, X_val, y_train, y_val = train_test_split(df['comments'], df['teacher/course'], test_size=0.2, stratify=df['teacher/course'], random_state=42)\n\ntfidf_lr = Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1,2), min_df=1)), ('clf', LogisticRegression(max_iter=200, class_weight='balanced'))])\ntfidf_lr.fit(X_train, y_train)\nprint(classification_report(y_val, tfidf_lr.predict(X_val)))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Feature inspection for explainability\nInspect top n-grams per class to validate reliance on meaningful cues rather than spurious patterns."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import numpy as np\nvec = tfidf_lr.named_steps['tfidf']\nclf = tfidf_lr.named_steps['clf']\nfeatures = np.array(vec.get_feature_names_out())\nfor i, cls in enumerate(clf.classes_):\n    top = clf.coef_[i].argsort()[-10:][::-1]\n    print(cls, features[top])"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Intermediate: Character n-grams\nRationale: robustness to spelling variations and low-resource slang."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from sklearn.feature_extraction.text import TfidfVectorizer as CharTfidf\nchar_model = Pipeline([('tfidf', CharTfidf(analyzer='char', ngram_range=(3,5), min_df=1)), ('clf', LogisticRegression(max_iter=200, class_weight='balanced'))])\nchar_model.fit(X_train, y_train)\nprint(classification_report(y_val, char_model.predict(X_val)))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Advanced: Transformer fine-tuning\nRationale: capture nuanced course/teacher cues beyond explicit keywords."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Transformer skeleton\n# from datasets import Dataset\n# from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n# tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n# label2id = {lbl:i for i,lbl in enumerate(sorted(df['teacher/course'].unique()))}\n# id2label = {i:lbl for lbl,i in label2id.items()}\n# dataset = Dataset.from_pandas(df[['comments', 'teacher/course']])\n# dataset = dataset.map(lambda x: {'labels': label2id[x['teacher/course']]}, remove_columns=['teacher/course'])\n# dataset = dataset.map(lambda x: tokenizer(x['comments'], truncation=True), batched=True)\n# model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(label2id), id2label=id2label, label2id=label2id)\n# args = TrainingArguments(output_dir='./teacher-course-model', evaluation_strategy='epoch', learning_rate=2e-5, num_train_epochs=8, per_device_train_batch_size=16)\n# trainer = Trainer(model=model, args=args, train_dataset=dataset, eval_dataset=dataset, tokenizer=tokenizer)\n# trainer.train()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Explainability\n- Token attributions (IG/SHAP) to highlight cues for each class.\n- Counterfactuals: minimally edit a teacher comment to look like course feedback and observe prediction flips."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Agentic routing and uncertainty\nRationale: combine rule-based confidence with model predictions.\n- Use heuristic confidence (e.g., keyword ratio) to decide whether to trust baseline or call LLM.\n- For low-confidence cases, ask LLM for classification + rationale; log both for auditability."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Placeholder for a simple router\n# def route_prediction(text):\n#     kw = keyword_baseline(text)\n#     score = max(tfidf_lr.predict_proba([text])[0])\n#     if score > 0.8:\n#         return {'label': tfidf_lr.predict([text])[0], 'source': 'tfidf', 'confidence': score}\n#     else:\n#         # call LLM fallback here\n#         return {'label': kw, 'source': 'llm_fallback', 'confidence': 0.5}\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. Error analysis & ethics\n- Inspect misclassifications grouped by presence/absence of explicit teacher/course terms.\n- Ethical considerations: avoid reinforcing gendered language; keep human-in-the-loop for high-stakes use."
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}