{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Aspect-Aware Sentiment Modeling for Educational Feedback\n",
        "Formalizes the research-exchange idea with runnable baselines, advanced models, prompting hooks, and explainability for joint aspect+sentiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n",
        "This notebook provides executable code blocks from data loading through training, evaluation, explainability, and CLI entry points. Install extras (`transformers`, `sentence-transformers`, `shap`, `lime`, `openai`) if they are not already available in your environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import shap\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "\n",
        "# Optional heavy deps (guarded imports)\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "except Exception:\n",
        "    SentenceTransformer = None\n",
        "\n",
        "try:\n",
        "    from transformers import (\n",
        "        AutoTokenizer,\n",
        "        AutoModelForSequenceClassification,\n",
        "        Trainer,\n",
        "        TrainingArguments,\n",
        "        DataCollatorWithPadding,\n",
        "    )\n",
        "    import datasets\n",
        "    from torch import nn\n",
        "    import torch\n",
        "except Exception:\n",
        "    AutoTokenizer = AutoModelForSequenceClassification = Trainer = TrainingArguments = DataCollatorWithPadding = None\n",
        "    datasets = nn = torch = None"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    data_path: Path = Path(\"data_feedback.xlsx\")\n",
        "    text_col: str = \"comments\"\n",
        "    aspect_col: str = \"teacher/course\"\n",
        "    label_col: str = \"sentiment\"\n",
        "    random_state: int = 42"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def load_data(cfg: Config) -> pd.DataFrame:\n",
        "    if cfg.data_path.exists():\n",
        "        df = pd.read_excel(cfg.data_path)\n",
        "    else:\n",
        "        df = pd.DataFrame(\n",
        "            {\n",
        "                \"teacher/course\": [\"teacher\", \"course\"],\n",
        "                \"comments\": [\"great teacher\", \"great course\"],\n",
        "                \"sentiment\": [\"positive\", \"positive\"],\n",
        "            }\n",
        "        )\n",
        "    df = df.rename(columns={cfg.text_col: \"text\", cfg.aspect_col: \"aspect_tag\", cfg.label_col: \"label\"})\n",
        "    df = df.dropna(subset=[\"text\", \"aspect_tag\", \"label\"]).reset_index(drop=True)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data audit and splits\n",
        "Includes aspect-specific splits to enable cross-aspect transfer evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def train_val_split(df: pd.DataFrame, cfg: Config):\n",
        "    strat = df[\"label\"] if df[\"label\"].nunique() > 1 else None\n",
        "    train_df, val_df = train_test_split(df, test_size=0.25, random_state=cfg.random_state, stratify=strat)\n",
        "    return train_df.reset_index(drop=True), val_df.reset_index(drop=True)\n",
        "\n",
        "cfg = Config()\n",
        "df = load_data(cfg)\n",
        "train_df, val_df = train_val_split(df, cfg)\n",
        "train_df.head(), val_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Preprocessing helpers and evaluation utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def prepend_aspect(texts: List[str], aspects: List[str]):\n",
        "    return [f\"[ASPECT={a}] {t}\" for t, a in zip(texts, aspects)]\n",
        "\n",
        "\n",
        "def evaluate(model, X_val, y_val, target_names=None, label=\"eval\"):\n",
        "    preds = model.predict(X_val)\n",
        "    report = classification_report(y_val, preds, target_names=target_names, zero_division=0)\n",
        "    cm = confusion_matrix(y_val, preds)\n",
        "    print(f\"\n",
        "==== {label} report ====\")\n",
        "    print(report)\n",
        "    print(\"Confusion matrix:\n",
        "\", cm)\n",
        "    return report, cm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. N-gram TF\u2013IDF baselines (word + character)\n",
        "Includes aspect prompts to test aspect-aware conditioning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def run_tfidf_baseline(train_df, val_df, use_aspect_prompt=False, analyzer=\"word\", ngram_range=(1, 2)):\n",
        "    X_train = train_df[\"text\"] if not use_aspect_prompt else prepend_aspect(train_df[\"text\"].tolist(), train_df[\"aspect_tag\"].tolist())\n",
        "    X_val = val_df[\"text\"] if not use_aspect_prompt else prepend_aspect(val_df[\"text\"].tolist(), val_df[\"aspect_tag\"].tolist())\n",
        "    y_train, y_val = train_df[\"label\"], val_df[\"label\"]\n",
        "\n",
        "    pipe = Pipeline(\n",
        "        [\n",
        "            (\"tfidf\", TfidfVectorizer(analyzer=analyzer, ngram_range=ngram_range, min_df=1)),\n",
        "            (\"clf\", LogisticRegression(max_iter=200, class_weight=\"balanced\")),\n",
        "        ]\n",
        "    )\n",
        "    pipe.fit(X_train, y_train)\n",
        "    return pipe, evaluate(pipe, X_val, y_val, label=f\"tfidf-{analyzer}-aspect={use_aspect_prompt}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Sentence-embedding classifier (SBERT + LogisticRegression)\n",
        "Uses aspect prompts to provide aspect-aware context."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def run_sbert_classifier(train_df, val_df, model_name: str = \"all-MiniLM-L6-v2\", use_aspect_prompt=True):\n",
        "    if SentenceTransformer is None:\n",
        "        raise ImportError(\"sentence-transformers not installed\")\n",
        "\n",
        "    model = SentenceTransformer(model_name)\n",
        "    X_train = train_df[\"text\"] if not use_aspect_prompt else prepend_aspect(train_df[\"text\"].tolist(), train_df[\"aspect_tag\"].tolist())\n",
        "    X_val = val_df[\"text\"] if not use_aspect_prompt else prepend_aspect(val_df[\"text\"].tolist(), val_df[\"aspect_tag\"].tolist())\n",
        "    y_train, y_val = train_df[\"label\"], val_df[\"label\"]\n",
        "\n",
        "    train_emb = model.encode(list(X_train), batch_size=16, show_progress_bar=True)\n",
        "    val_emb = model.encode(list(X_val), batch_size=16, show_progress_bar=True)\n",
        "\n",
        "    clf = LogisticRegression(max_iter=200, class_weight=\"balanced\")\n",
        "    clf.fit(train_emb, y_train)\n",
        "    evaluate(clf, val_emb, y_val, label=f\"sbert-aspect={use_aspect_prompt}\")\n",
        "    return clf, model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Transformer fine-tuning (aspect-prompted)\n",
        "Lightweight Trainer setup for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def prepare_hf_dataset(train_df, val_df, tokenizer):\n",
        "    def tokenize(batch):\n",
        "        texts = prepend_aspect(batch[\"text\"], batch[\"aspect_tag\"])\n",
        "        return tokenizer(texts, truncation=True, max_length=256)\n",
        "\n",
        "    train_ds = datasets.Dataset.from_pandas(train_df)\n",
        "    val_ds = datasets.Dataset.from_pandas(val_df)\n",
        "    return train_ds.map(tokenize, batched=True), val_ds.map(tokenize, batched=True)\n",
        "\n",
        "\n",
        "def run_transformer(train_df, val_df, model_name=\"distilbert-base-uncased\", num_epochs=3):\n",
        "    if AutoTokenizer is None:\n",
        "        raise ImportError(\"transformers not installed\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=train_df[\"label\"].nunique())\n",
        "\n",
        "    train_ds, val_ds = prepare_hf_dataset(train_df, val_df, tokenizer)\n",
        "    collator = DataCollatorWithPadding(tokenizer)\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=\"./outputs\",\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        num_train_epochs=num_epochs,\n",
        "        weight_decay=0.01,\n",
        "        logging_steps=50,\n",
        "    )\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        preds, labels = eval_pred\n",
        "        pred_labels = preds.argmax(axis=1)\n",
        "        report = classification_report(labels, pred_labels, output_dict=True, zero_division=0)\n",
        "        return {\"macro_f1\": report[\"macro avg\"][\"f1-score\"]}\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=val_ds,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "    trainer.train()\n",
        "    return trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Multi-task (aspect + sentiment) head for joint learning\n",
        "Shared encoder with dual classification heads to exploit aspect cues when predicting sentiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class MultiTaskHead(nn.Module):\n",
        "    def __init__(self, hidden_size, num_sentiment, num_aspect):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.sentiment_classifier = nn.Linear(hidden_size, num_sentiment)\n",
        "        self.aspect_classifier = nn.Linear(hidden_size, num_aspect)\n",
        "\n",
        "    def forward(self, features, **kwargs):\n",
        "        x = self.dropout(features)\n",
        "        return {\n",
        "            \"sentiment\": self.sentiment_classifier(x),\n",
        "            \"aspect\": self.aspect_classifier(x),\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Cross-aspect robustness (train on teacher, test on course and vice versa)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def cross_aspect_eval(df, model_fn):\n",
        "    teacher_df = df[df[\"aspect_tag\"] == \"teacher\"].reset_index(drop=True)\n",
        "    course_df = df[df[\"aspect_tag\"] == \"course\"].reset_index(drop=True)\n",
        "    results = {}\n",
        "    for train_df, test_df, tag in [\n",
        "        (teacher_df, course_df, \"train_teacher_test_course\"),\n",
        "        (course_df, teacher_df, \"train_course_test_teacher\"),\n",
        "    ]:\n",
        "        model, _ = model_fn(train_df, test_df)\n",
        "        results[tag] = model\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Lightweight data augmentation for robustness\n",
        "Simple synonym/word-drop augmentations; plug into any experiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def random_drop(text, p=0.15):\n",
        "    words = text.split()\n",
        "    keep = [w for w in words if random.random() > p]\n",
        "    return \" \".join(keep) if keep else text\n",
        "\n",
        "\n",
        "def augment_dataframe(df, times=1):\n",
        "    rows = []\n",
        "    for _ in range(times):\n",
        "        for _, row in df.iterrows():\n",
        "            rows.append({\n",
        "                \"text\": random_drop(row[\"text\"]),\n",
        "                \"aspect_tag\": row[\"aspect_tag\"],\n",
        "                \"label\": row[\"label\"],\n",
        "            })\n",
        "    return pd.concat([df, pd.DataFrame(rows)], ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Zero/low-shot prompting baseline (LLM)\n",
        "Uses explicit schema and aspect cues; keep API keys in environment variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def prompt_sentiment(texts: List[str], aspects: List[str], model_name: str = \"gpt-4o-mini\"):\n",
        "    import openai\n",
        "\n",
        "    client = openai.OpenAI()\n",
        "    outputs = []\n",
        "    for t, a in zip(texts, aspects):\n",
        "        res = client.chat.completions.create(\n",
        "            model=model_name,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"Classify sentiment as positive, neutral, or negative and explain briefly.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Aspect: {a}. Comment: {t}\"},\n",
        "            ],\n",
        "            temperature=0,\n",
        "        )\n",
        "        outputs.append(res.choices[0].message[\"content\"])\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Error analysis and explainability reports\n",
        "Combine SHAP/LIME outputs with per-length/per-aspect slices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def error_table(model, val_df, use_aspect_prompt=True):\n",
        "    X_val = val_df[\"text\"] if not use_aspect_prompt else prepend_aspect(val_df[\"text\"].tolist(), val_df[\"aspect_tag\"].tolist())\n",
        "    y_true = val_df[\"label\"].tolist()\n",
        "    preds = model.predict(X_val)\n",
        "    df_err = val_df.copy()\n",
        "    df_err[\"pred\"] = preds\n",
        "    df_err[\"correct\"] = df_err[\"pred\"] == df_err[\"label\"]\n",
        "    return df_err.sort_values(\"correct\")\n",
        "\n",
        "\n",
        "def explain_with_shap(model, X_samples: List[str], class_names: List[str]):\n",
        "    explainer = shap.Explainer(model.predict_proba, masker=shap.maskers.Text())\n",
        "    shap_values = explainer(X_samples)\n",
        "    shap.plots.text(shap_values, display=False)\n",
        "    return shap_values\n",
        "\n",
        "\n",
        "def explain_with_lime(model, X_samples: List[str], class_names: List[str]):\n",
        "    explainer = LimeTextExplainer(class_names=class_names)\n",
        "    explanations = [explainer.explain_instance(x, model.predict_proba, num_features=8) for x in X_samples]\n",
        "    return explanations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. CLI entry points\n",
        "Run from terminal: `python -m sentiment_analysis --model tfidf` etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def main_cli():\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Aspect-aware sentiment experiments\")\n",
        "    parser.add_argument(\"--model\", choices=[\"tfidf\", \"char\", \"sbert\", \"transformer\"], default=\"tfidf\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    cfg = Config()\n",
        "    df = load_data(cfg)\n",
        "    train_df, val_df = train_val_split(df, cfg)\n",
        "\n",
        "    if args.model == \"tfidf\":\n",
        "        run_tfidf_baseline(train_df, val_df, use_aspect_prompt=False)\n",
        "    elif args.model == \"char\":\n",
        "        run_tfidf_baseline(train_df, val_df, use_aspect_prompt=True, analyzer=\"char\", ngram_range=(3, 5))\n",
        "    elif args.model == \"sbert\":\n",
        "        run_sbert_classifier(train_df, val_df)\n",
        "    elif args.model == \"transformer\":\n",
        "        run_transformer(train_df, val_df)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_cli()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}