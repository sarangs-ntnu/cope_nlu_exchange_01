{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Aspect-Aware Sentiment Modeling for Educational Feedback\n",
        "\n",
        "Formalizes the research-exchange idea with baselines, transformers, prompting, robustness, and explainability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "DATA_PATH = Path(\"data_feedback.xlsx\")\n",
        "RANDOM_SEED = 42"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load and audit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df = pd.read_excel(DATA_PATH)\n",
        "df = df.rename(columns={\"teacher/course\": \"topic\"})\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Class distribution (sentiment):\")\n",
        "print(df[\"sentiment\"].value_counts())\n",
        "print(\"\n",
        "Topic distribution:\")\n",
        "print(df[\"topic\"].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "train_df, val_df = train_test_split(df, test_size=0.25, stratify=df[\"sentiment\"], random_state=RANDOM_SEED)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Helpers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def evaluate(model, x_train, y_train, x_val, y_val, label):\n",
        "    model.fit(x_train, y_train)\n",
        "    preds = model.predict(x_val)\n",
        "    print(f\"=== {label} ===\")\n",
        "    print(classification_report(y_val, preds))\n",
        "    cm = confusion_matrix(y_val, preds, labels=sorted(y_val.unique()))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=sorted(y_val.unique()), yticklabels=sorted(y_val.unique()))\n",
        "    plt.title(label)\n",
        "    plt.xlabel(\"Pred\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.show()\n",
        "    return preds\n",
        "\n",
        "def build_aspect_prompt(texts, aspects):\n",
        "    return [f\"[ASPECT={a}] {t}\" for a, t in zip(aspects, texts)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Word TF-IDF baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "word_clf = Pipeline([(\"tfidf\", TfidfVectorizer(ngram_range=(1,2), min_df=1)), (\"clf\", LogisticRegression(max_iter=1000, class_weight=\"balanced\"))])\n",
        "_ = evaluate(word_clf, train_df[\"comments\"], train_df[\"sentiment\"], val_df[\"comments\"], val_df[\"sentiment\"], \"Word TF-IDF\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Character TF-IDF baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "char_clf = Pipeline([(\"tfidf\", TfidfVectorizer(analyzer=\"char\", ngram_range=(3,5), min_df=1)), (\"clf\", LogisticRegression(max_iter=200, class_weight=\"balanced\"))])\n",
        "_ = evaluate(char_clf, train_df[\"comments\"], train_df[\"sentiment\"], val_df[\"comments\"], val_df[\"sentiment\"], \"Char TF-IDF\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Aspect-prompted TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "prompted_train = build_aspect_prompt(train_df[\"comments\"], train_df[\"aspect\"])\n",
        "prompted_val = build_aspect_prompt(val_df[\"comments\"], val_df[\"aspect\"])\n",
        "asp_word_clf = Pipeline([(\"tfidf\", TfidfVectorizer(ngram_range=(1,2), min_df=1)), (\"clf\", LogisticRegression(max_iter=1000, class_weight=\"balanced\"))])\n",
        "_ = evaluate(asp_word_clf, prompted_train, train_df[\"sentiment\"], prompted_val, val_df[\"sentiment\"], \"Aspect-prompted TF-IDF\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Sentence-embedding classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "embedder = SentenceTransformer(\"all-mpnet-base-v2\")\n",
        "train_vecs = embedder.encode(train_df[\"comments\"].tolist(), show_progress_bar=False)\n",
        "val_vecs = embedder.encode(val_df[\"comments\"].tolist(), show_progress_bar=False)\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "train_vecs_scaled = scaler.fit_transform(train_vecs)\n",
        "val_vecs_scaled = scaler.transform(val_vecs)\n",
        "embed_clf = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
        "embed_clf.fit(train_vecs_scaled, train_df[\"sentiment\"])\n",
        "preds = embed_clf.predict(val_vecs_scaled)\n",
        "print(classification_report(val_df[\"sentiment\"], preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Transformer fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "label2id = {l:i for i, l in enumerate(sorted(df[\"sentiment\"].unique()))}\n",
        "id2label = {i:l for l, i in label2id.items()}\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "train_ds = Dataset.from_dict({\"text\": train_df[\"comments\"].tolist(), \"label\": [label2id[x] for x in train_df[\"sentiment\"]]})\n",
        "val_ds = Dataset.from_dict({\"text\": val_df[\"comments\"].tolist(), \"label\": [label2id[x] for x in val_df[\"sentiment\"]]})\n",
        "train_ds = train_ds.map(tokenize, batched=True)\n",
        "val_ds = val_ds.map(tokenize, batched=True)\n",
        "train_ds.set_format(\"torch\")\n",
        "val_ds.set_format(\"torch\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label2id), id2label=id2label, label2id=label2id)\n",
        "args = TrainingArguments(output_dir=\"./sentiment_distilbert\", learning_rate=2e-5, per_device_train_batch_size=8, per_device_eval_batch_size=16, num_train_epochs=5, evaluation_strategy=\"epoch\", save_strategy=\"epoch\", load_best_model_at_end=True, weight_decay=0.01, logging_steps=10)\n",
        "trainer = Trainer(model=model, args=args, train_dataset=train_ds, eval_dataset=val_ds, tokenizer=tokenizer)\n",
        "# trainer.train()\n",
        "# trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Aspect-aware transformer (multi-task head)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from transformers import AutoModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultiTaskModel(nn.Module):\n",
        "    def __init__(self, base_model_name, num_sentiment, num_aspect):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(base_model_name)\n",
        "        hidden = self.encoder.config.hidden_size\n",
        "        self.dropout = nn.Dropout(self.encoder.config.dropout)\n",
        "        self.sentiment_head = nn.Linear(hidden, num_sentiment)\n",
        "        self.aspect_head = nn.Linear(hidden, num_aspect)\n",
        "    def forward(self, input_ids=None, attention_mask=None, labels=None, aspect_labels=None):\n",
        "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled = outputs.last_hidden_state[:, 0]\n",
        "        pooled = self.dropout(pooled)\n",
        "        sent_logits = self.sentiment_head(pooled)\n",
        "        asp_logits = self.aspect_head(pooled)\n",
        "        loss = None\n",
        "        if labels is not None and aspect_labels is not None:\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(sent_logits, labels) + loss_fn(asp_logits, aspect_labels)\n",
        "        return {\"loss\": loss, \"logits\": sent_logits, \"aspect_logits\": asp_logits}\n",
        "\n",
        "asp_label2id = {l:i for i, l in enumerate(sorted(df[\"aspect\"].unique()))}\n",
        "mt_model = MultiTaskModel(model_name, num_sentiment=len(label2id), num_aspect=len(asp_label2id))\n",
        "# To train, subclass Trainer to pass both labels."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Cross-aspect generalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "teacher_df = df[df[\"topic\"] == \"teacher\"]\n",
        "course_df = df[df[\"topic\"] == \"course\"]\n",
        "teacher_train, teacher_val = train_test_split(teacher_df, test_size=0.3, random_state=RANDOM_SEED)\n",
        "course_train, course_val = train_test_split(course_df, test_size=0.3, random_state=RANDOM_SEED)\n",
        "transfer_clf = Pipeline([(\"tfidf\", TfidfVectorizer(ngram_range=(1,2))), (\"clf\", LogisticRegression(max_iter=1000, class_weight=\"balanced\"))])\n",
        "transfer_clf.fit(teacher_train[\"comments\"], teacher_train[\"sentiment\"])\n",
        "course_preds = transfer_clf.predict(course_val[\"comments\"])\n",
        "print(\"Teacher->Course transfer\")\n",
        "print(classification_report(course_val[\"sentiment\"], course_preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Data augmentation (synonyms)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import random\n",
        "synonym_map = {\"good\": [\"great\", \"nice\"], \"great\": [\"excellent\"], \"helpful\": [\"supportive\"], \"humor\": [\"wit\"], \"knowledge\": [\"expertise\"]}\n",
        "\n",
        "def augment_comment(text):\n",
        "    tokens = text.split()\n",
        "    augmented = []\n",
        "    for tok in tokens:\n",
        "        key = tok.lower().strip(\".,\")\n",
        "        if key in synonym_map and random.random() < 0.3:\n",
        "            augmented.append(random.choice(synonym_map[key]))\n",
        "        else:\n",
        "            augmented.append(tok)\n",
        "    return \" \".join(augmented)\n",
        "augmented_df = train_df.copy()\n",
        "augmented_df[\"comments\"] = augmented_df[\"comments\"].apply(augment_comment)\n",
        "augmented_df[\"is_aug\"] = True\n",
        "combined = pd.concat([train_df.assign(is_aug=False), augmented_df])\n",
        "combined.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Zero/low-shot prompting baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from transformers import pipeline\n",
        "prompt_template = \"Classify the sentiment (positive/neutral/negative) of this student feedback about {aspect} and explain briefly: {text}\"\n",
        "zero_shot_clf = pipeline(\"text-classification\", model=\"facebook/bart-large-mnli\")\n",
        "example = val_df.iloc[0]\n",
        "text = prompt_template.format(aspect=example[\"aspect\"], text=example[\"comments\"])\n",
        "print(zero_shot_clf(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Error analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def collect_errors(model, x_val, y_val):\n",
        "    preds = model.predict(x_val)\n",
        "    return pd.DataFrame({\"text\": x_val, \"true\": y_val, \"pred\": preds}).query(\"true != pred\")\n",
        "err_df = collect_errors(word_clf, val_df[\"comments\"], val_df[\"sentiment\"])\n",
        "err_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. CLI baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import argparse\n",
        "\n",
        "def run_cli():\n",
        "    parser = argparse.ArgumentParser(description=\"Aspect-aware sentiment experiments\")\n",
        "    parser.add_argument(\"--use_aspect\", action=\"store_true\", help=\"prepend aspect tokens\")\n",
        "    args = parser.parse_args(args=[])\n",
        "    x_train = train_df[\"comments\"] if not args.use_aspect else build_aspect_prompt(train_df[\"comments\"], train_df[\"aspect\"])\n",
        "    x_val = val_df[\"comments\"] if not args.use_aspect else build_aspect_prompt(val_df[\"comments\"], val_df[\"aspect\"])\n",
        "    model = Pipeline([(\"tfidf\", TfidfVectorizer(ngram_range=(1,2))), (\"clf\", LogisticRegression(max_iter=1000, class_weight=\"balanced\"))])\n",
        "    evaluate(model, x_train, train_df[\"sentiment\"], x_val, val_df[\"sentiment\"], \"CLI run\")\n",
        "\n",
        "# run_cli()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}