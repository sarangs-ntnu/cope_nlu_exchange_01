{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Sentiment Analysis Experiments (basic \u2192 advanced \u2192 cross-domain)\n",
        "Structured experiments to classify sentiment (`positive/neutral/negative`) with progressively richer models, explainability, and domain generalization tests.\n"
      ],
      "execution_count": null,
      "outputs": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_PATH = Path('data_feedback.xlsx')\n",
        "df = pd.read_excel(DATA_PATH)\n",
        "print(df.head())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "# Basic dataset audit and safeguards\n",
        "print('Rows:', len(df))\n",
        "print('Columns:', df.columns.tolist())\n",
        "print('\n",
        "Label distribution:')\n",
        "print(df['sentiment'].value_counts())\n",
        "print('\n",
        "Comment length stats:')\n",
        "print(df['comments'].str.len().describe())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "# Train/validation split with stratification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    df['comments'], df['sentiment'], test_size=0.3, random_state=42, stratify=df['sentiment']\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 1. Classic baseline: word TF\u2013IDF + Logistic Regression\n",
        "Transparent benchmark; quick to train and easy to explain. Includes calibration-style probability outputs.\n"
      ],
      "execution_count": null,
      "outputs": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "word_lr = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(ngram_range=(1,2), min_df=1)),\n",
        "    ('clf', LogisticRegression(max_iter=200, class_weight='balanced'))\n",
        "])\n",
        "word_lr.fit(X_train, y_train)\n",
        "y_pred = word_lr.predict(X_val)\n",
        "print(classification_report(y_val, y_pred))\n",
        "ConfusionMatrixDisplay.from_predictions(y_val, y_pred, normalize='true', cmap='Blues')\n",
        "plt.title('Word TF\u2013IDF baseline')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Explainability for baseline (SHAP)\n",
        "Identify influential n-grams driving predictions to support responsible deployment.\n"
      ],
      "execution_count": null,
      "outputs": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "explainer = shap.LinearExplainer(word_lr.named_steps['clf'], word_lr.named_steps['tfidf'].transform(X_train))\n",
        "val_tfidf = word_lr.named_steps['tfidf'].transform(X_val)\n",
        "shap_values = explainer(val_tfidf)\n",
        "# Visualize a single example\n",
        "sample_idx = 0\n",
        "shap.plots.text(shap.Explanation(values=shap_values[sample_idx].toarray()[0],\n",
        "                                 data=word_lr.named_steps['tfidf'].inverse_transform(val_tfidf[sample_idx])[0]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 2. Character n-grams for robustness\n",
        "Helps with spelling mistakes and stylized text. Useful for noisy student comments.\n"
      ],
      "execution_count": null,
      "outputs": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer as CharTfidf\n",
        "\n",
        "char_lr = Pipeline([\n",
        "    ('tfidf', CharTfidf(analyzer='char', ngram_range=(3,5), min_df=1)),\n",
        "    ('clf', LogisticRegression(max_iter=200, class_weight='balanced'))\n",
        "])\n",
        "char_lr.fit(X_train, y_train)\n",
        "char_pred = char_lr.predict(X_val)\n",
        "print(classification_report(y_val, char_pred))\n",
        "ConfusionMatrixDisplay.from_predictions(y_val, char_pred, normalize='true', cmap='Purples')\n",
        "plt.title('Character TF\u2013IDF baseline')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 3. Cross-domain and generalization probes\n",
        "Train on teacher feedback and test on course feedback (and vice versa) to check domain leakage and robustness.\n"
      ],
      "execution_count": null,
      "outputs": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "teacher_mask = df['teacher/course'].str.lower().eq('teacher')\n",
        "course_mask = df['teacher/course'].str.lower().eq('course')\n",
        "\n",
        "def cross_domain_score(train_mask, test_mask):\n",
        "    model = word_lr\n",
        "    model.fit(df.loc[train_mask, 'comments'], df.loc[train_mask, 'sentiment'])\n",
        "    preds = model.predict(df.loc[test_mask, 'comments'])\n",
        "    return accuracy_score(df.loc[test_mask, 'sentiment'], preds)\n",
        "\n",
        "teacher_to_course = cross_domain_score(teacher_mask, course_mask)\n",
        "course_to_teacher = cross_domain_score(course_mask, teacher_mask)\n",
        "print({'train_teacher_test_course': teacher_to_course, 'train_course_test_teacher': course_to_teacher})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Data augmentation (noise injection) for domain robustness\n",
        "A simple synonym/dropout augmenter to reduce overfitting to phrasing.\n"
      ],
      "execution_count": null,
      "outputs": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "import random\n",
        "\n",
        "def word_dropout(text, p=0.1):\n",
        "    tokens = text.split()\n",
        "    kept = [t for t in tokens if random.random() > p]\n",
        "    return ' '.join(kept) if kept else text\n",
        "\n",
        "a_aug = df.copy()\n",
        "a_aug['comments'] = a_aug['comments'].apply(lambda t: word_dropout(t, p=0.15))\n",
        "\n",
        "X_train_aug, X_val_aug, y_train_aug, y_val_aug = train_test_split(\n",
        "    a_aug['comments'], a_aug['sentiment'], test_size=0.3, random_state=1, stratify=a_aug['sentiment']\n",
        ")\n",
        "word_lr_aug = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(ngram_range=(1,2), min_df=1)),\n",
        "    ('clf', LogisticRegression(max_iter=200, class_weight='balanced'))\n",
        "])\n",
        "word_lr_aug.fit(X_train_aug, y_train_aug)\n",
        "print(classification_report(y_val_aug, word_lr_aug.predict(X_val_aug)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 4. Transformer fine-tuning (small dataset aware)\n",
        "Fine-tune `distilbert-base-uncased` with weighted loss; keep epochs/learning rate low for small data. Can be swapped with larger models if GPU is available.\n"
      ],
      "execution_count": null,
      "outputs": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "\n",
        "model_name = 'distilbert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "train_ds = Dataset.from_pandas(pd.DataFrame({'text': X_train, 'label': y_train.replace({'positive':2,'neutral':1,'negative':0})}))\n",
        "val_ds = Dataset.from_pandas(pd.DataFrame({'text': X_val, 'label': y_val.replace({'positive':2,'neutral':1,'negative':0})}))\n",
        "\n",
        "label_map = {0:'negative',1:'neutral',2:'positive'}\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch['text'], padding='max_length', truncation=True, max_length=128)\n",
        "\n",
        "train_ds = train_ds.map(tokenize, batched=True)\n",
        "val_ds = val_ds.map(tokenize, batched=True)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir='sentiment-model',\n",
        "    evaluation_strategy='epoch',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    weight_decay=0.01,\n",
        "    learning_rate=2e-5,\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "    from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {'accuracy': acc, 'macro_f1': f1, 'precision': precision, 'recall': recall}\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "# trainer.train()  # Uncomment to fine-tune when resources are available\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Zero-shot / few-shot LLM probe\n",
        "Uses an instruction/zero-shot classifier to benchmark label-space alignment without training.\n"
      ],
      "execution_count": null,
      "outputs": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "zs_classifier = pipeline('zero-shot-classification', model='facebook/bart-large-mnli')\n",
        "labels = ['positive','neutral','negative']\n",
        "text_example = df['comments'].iloc[0]\n",
        "print(zs_classifier(text_example, candidate_labels=labels, hypothesis_template='This review is {label}.'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 5. Agentic routing: ensemble of baseline + LLM\n",
        "Combine calibrated baseline with zero-shot LLM when confidence is low.\n"
      ],
      "execution_count": null,
      "outputs": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "word_lr_prob = word_lr.predict_proba(X_val)\n",
        "threshold = 0.55\n",
        "\n",
        "ensemble_preds = []\n",
        "for text, probs in zip(X_val, word_lr_prob):\n",
        "    max_p = probs.max()\n",
        "    if max_p >= threshold:\n",
        "        ensemble_preds.append(word_lr.classes_[probs.argmax()])\n",
        "    else:\n",
        "        zs = zs_classifier(text, candidate_labels=labels, hypothesis_template='This review is {label}.')\n",
        "        ensemble_preds.append(zs['labels'][0])\n",
        "\n",
        "print('Ensembled predictions for first 5 examples:')\n",
        "for t,p in zip(X_val[:5], ensemble_preds[:5]):\n",
        "    print(p, '->', t)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 6. Error analysis and ethics checklist\n",
        "Creates a table of misclassifications and tags likely causes (length, domain, class confusion) to guide mitigation.\n"
      ],
      "execution_count": null,
      "outputs": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "val_df = pd.DataFrame({'text': X_val, 'true': y_val, 'pred': y_pred})\n",
        "errors = val_df[val_df['true'] != val_df['pred']].copy()\n",
        "errors['length'] = errors['text'].str.len()\n",
        "errors['domain'] = errors['text'].apply(lambda t: 'teacher' if 'teacher' in t.lower() else 'course' if 'course' in t.lower() else 'unknown')\n",
        "print(errors[['text','true','pred','length','domain']])\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}